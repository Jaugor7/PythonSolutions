{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Important Modules\n",
    "import csv                                #To Open Csv Output File\n",
    "import tldextract                         #To Extract Top level Domain of our URL\n",
    "import requests                           #To Provide our script with Internet Connectivity \n",
    "from bs4 import BeautifulSoup             #To Extract our DOM(Document Object Model)-Tree of our URL\n",
    "from urlextract import URLExtract         #Extract All URL's  from our given URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extract_Features:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.extracted_features = []\n",
    "        self.count = 0\n",
    "        self.url = url\n",
    "        \n",
    "        #Feature 1\n",
    "    def __url_features(self):\n",
    "        flag = 0\n",
    "        \n",
    "        if self.url.count('.') >= 4:\n",
    "            flag = 1\n",
    "            \n",
    "        self.extracted_features.append(flag)\n",
    "        self.count += 1\n",
    "        \n",
    "        #Feature 2\n",
    "    def __special_char(self):\n",
    "        flag = 0\n",
    "        \n",
    "        if self.url.find('@') >= 0 or self.url.find('-'):\n",
    "            flag = 1\n",
    "            \n",
    "        self.extracted_features.append(flag)\n",
    "        self.count += 1\n",
    "        \n",
    "        #Feature 3\n",
    "    def __url_len(self):\n",
    "        flag = 0\n",
    "        \n",
    "        if len(self.url) >= 74:\n",
    "            flag = 1\n",
    "            \n",
    "        self.extracted_features.append(flag)\n",
    "        self.count += 1 \n",
    "        \n",
    "        #Feature 4\n",
    "    def __susp_word(self):\n",
    "        susp_wrds = [\"security\",\"signin\",\"login\",\"bank\",\"account\",\"update\",\"include\",\"webs\",\"online\",\"Secur\",\"Verif\",\"Com-\",\"Support\",\"Service\",\"Auth\",\"Confirm\",\"Account\"]\n",
    "        flag = 0\n",
    "        \n",
    "        for word in susp_wrds:\n",
    "            if word in self.url:\n",
    "                flag = 1\n",
    "                break\n",
    "                \n",
    "        self.extracted_features.append(flag)\n",
    "        self.count += 1\n",
    "        \n",
    "        #Feature 5\n",
    "    def __tld_count(self):\n",
    "        susp_wrds = ['.com','.org','.edu','.biz','.gov','.net']\n",
    "        flag = 0\n",
    "        \n",
    "        for word in susp_wrds:\n",
    "            if self.url.count(word) > 1:\n",
    "                flag = 1\n",
    "                break\n",
    "                \n",
    "        self.extracted_features.append(flag)\n",
    "        self.count += 1\n",
    "    \n",
    "        #Feature 6\n",
    "    def __http_count(self):\n",
    "        flag = 0\n",
    "        \n",
    "        if self.url.count('http') > 1:\n",
    "            flag = 1\n",
    "            \n",
    "        self.extracted_features.append(flag)\n",
    "        self.count += 1\n",
    "        \n",
    "        #Feature 7\n",
    "    def __brand_name(self):\n",
    "        flag = 0\n",
    "        susp_wrds = [\"Facebook\",\"Chevrolet\",\"Apple\",\"Google\",\"Microsoft\",\"Amazon\",\"PayPal\",\"Whatsapp\",\"Dropbox\",\"Paytm\",\"americanexpress\",\"Yahoo\",\"AOL\",\"USAA\"]\n",
    "        \n",
    "        for word in susp_wrds:\n",
    "            if self.url.find('word') > 25:\n",
    "                flag = 1\n",
    "                break\n",
    "                \n",
    "        self.extracted_features.append(flag)\n",
    "        self.count += 1\n",
    "            \n",
    "        #Feature 8\n",
    "    def __data_uri(self):\n",
    "        flag = 0\n",
    "        \n",
    "        if self.url.find(\"data:\"):\n",
    "            flag = 1\n",
    "            \n",
    "        self.extracted_features.append(flag)\n",
    "        self.count += 1\n",
    "        \n",
    "        #Feature 9\n",
    "    def __netfeature(self):\n",
    "        try:\n",
    "            result = requests.get(self.url)\n",
    "            src = result.content\n",
    "            soup = BeautifulSoup(src,'lxml')\n",
    "            form = soup.find_all('form')\n",
    "            \n",
    "            flag = 0\n",
    "            \n",
    "            for a in form:\n",
    "                \n",
    "                php = a.attrs['action']\n",
    "                \n",
    "                if php.find('.php') > -1 or php.find('#') > -1 or php.find('void(0)') > -1:\n",
    "                    flag = 1\n",
    "                    break\n",
    "                    \n",
    "            self.extracted_features.append(flag)\n",
    "            self.count += 1\n",
    "            \n",
    "        #Feature 10\n",
    "            flag = 0\n",
    "            html = soup.prettify()\n",
    "            extractor = URLExtract()\n",
    "            \n",
    "            links = extractor.find_urls(html)\n",
    "            no = len(links)\n",
    "            \n",
    "            if no == 0:\n",
    "                flag = 1\n",
    "                \n",
    "            self.extracted_features.append(flag)\n",
    "            self.count += 1\n",
    "            \n",
    "        #Feature 11\n",
    "            flag, fcount , empty_url_count, err_count, redirect_count = 0, 0, 0, 0, 0\n",
    "            \n",
    "            mdomain = tldextract.extract(self.url).domain\n",
    "            \n",
    "            for link in links:\n",
    "                \n",
    "                if link == '#' or link == '#content' or link == 'JavaScript::void(0)' or link == '#skip':\n",
    "                    empty_url_count += 1\n",
    "                    \n",
    "                domain = tldextract.extract(link).domain\n",
    "                if domain != mdomain:\n",
    "                    fcount += 1\n",
    "                \n",
    "                try:\n",
    "                    link = link.lower()\n",
    "                    if 'http' not in link:\n",
    "                        link = 'http://' + link\n",
    "                        \n",
    "                    r = requests.get(link).status_code\n",
    "                    \n",
    "                    if r == 404 or r == 403:\n",
    "                        err_count += 1\n",
    "                        \n",
    "                    elif r == 301 or r == 302:\n",
    "                        redirect_count += 1\n",
    "                    \n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "            if no > 0 and (fcount/no) > 0.5:\n",
    "                flag = 1\n",
    "                \n",
    "            self.extracted_features.append(flag)\n",
    "            self.count += 1\n",
    "            \n",
    "        #Feature 12\n",
    "            flag = 0\n",
    "            \n",
    "            if no > 0 and (empty_url_count/no) > 0.34:\n",
    "                flag = 1\n",
    "                \n",
    "            self.extracted_features.append(flag)\n",
    "            self.count += 1\n",
    "            \n",
    "        #Feature 13\n",
    "            flag = 0\n",
    "            tag = soup.find_all('link')\n",
    "            \n",
    "            if len(tag) == 1:\n",
    "                flag = 1\n",
    "                \n",
    "            self.extracted_features.append(flag)\n",
    "            self.count += 1\n",
    "            \n",
    "        #Feature 14\n",
    "            flag = 0\n",
    "            \n",
    "            if no > 0 and (err_count/no) > 0.3:\n",
    "                flag = 1\n",
    "                \n",
    "            self.extracted_features.append(flag)\n",
    "            self.count += 1\n",
    "            \n",
    "        #Feature 15\n",
    "            flag = 0\n",
    "            \n",
    "            if no > 0 and (redirect_count/no) > 0.3:\n",
    "                flag = 1\n",
    "                \n",
    "            self.extracted_features.append(flag)\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            for i in range(15 - self.count):\n",
    "                self.extracted_features.append(-1)\n",
    "                \n",
    "    def Extract(self):\n",
    "        self.__url_features()\n",
    "        self.__special_char()\n",
    "        self.__url_len()\n",
    "        self.__susp_word()\n",
    "        self.__tld_count()\n",
    "        self.__http_count()\n",
    "        self.__brand_name()\n",
    "        self.__data_uri()\n",
    "        self.__netfeature()\n",
    "        \n",
    "        return self.extracted_features\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'url.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ea381eaee01c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwritefile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"url.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'url.txt'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    with open(\"dataset.csv\", \"w\") as writefile:\n",
    "\n",
    "        writer = csv.writer(writefile)\n",
    "        reader = open(\"url.txt\",\"r\")\n",
    "\n",
    "        for link in reader:\n",
    "            link = link.lower()\n",
    "            \n",
    "            if 'http' not in link:\n",
    "                link = 'http://www.'+ link\n",
    "                \n",
    "            link = link.split('com')[0]+ 'com/'\n",
    "            \n",
    "            obj = Extract_Features(link)\n",
    "            \n",
    "            li = [link]\n",
    "            li.append(obj.Extract())\n",
    "            \n",
    "            print(li)\n",
    "            writer.writerow(li)\n",
    "\n",
    "        reader.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
